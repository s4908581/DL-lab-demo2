{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1rNYPiD_iV-9pFBs-Top83xq02d91U9bg","authorship_tag":"ABX9TyPZj6PMnf8NwphndCCq3d8X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pFknyc4NVuyP"},"outputs":[],"source":["from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#note: the discriminator only works for image size 256 x 256, be sure to change input dimensions if working with a different image\n","#generator is designed to take a latent space of 256\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","\n","def define_generator():\n","    input_layer = Input(shape=(256,))\n","\n","    # Initial fully connected layer\n","    x = Dense(32 * 32 * 256)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Reshape((32, 32, 256))(x)\n","\n","    # Transpose Convolution layer 2\n","    x = Conv2DTranspose(384, (3,3), strides=(2,2), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    # Transpose Convolution layer 2\n","    x = Conv2DTranspose(256, (3,3), strides=(2,2), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    # Transpose Convolution layer 3\n","    x = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    # Residual Connection\n","    shortcut = x\n","    x = Conv2D(128, (3,3), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Add()([shortcut, x])\n","\n","    # Output layer\n","    x = Conv2D(1, (3,3), strides=(1,1), padding='same', use_bias=False)(x)\n","    output = Activation('tanh')(x)\n","\n","    return Model(inputs=input_layer, outputs=output)\n","\n","###defining the discriminator###\n","def define_discriminator(input=(256,256,1)):\n","\n","    model = tf.keras.Sequential()\n","\n","    model.add(Conv2D(32, (4,4), strides=(2, 2), padding='same',input_shape=input))\n","    # model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.3))\n","\n","\n","    model.add(Conv2D(64, (4,4), strides=(2, 2), padding='same'))\n","    # model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(128, (4,4), strides=(2, 2), padding='same'))\n","    # model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Flatten())\n","    model.add(Dense(1))\n","    return model\n"],"metadata":{"id":"VWbPJAZ_29Bo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding: utf-8\n","\n","# In[ ]:\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import *\n","\n","import numpy as np\n","\n","from matplotlib import *\n","from matplotlib import pyplot\n","from matplotlib.pyplot import *\n","\n","from PIL import Image\n","\n","import glob\n","import os\n","import sys\n","\n","from skimage import metrics\n","from skimage.metrics import structural_similarity as ssim\n","\n","\n","#check if GPU is available\n","tf.config.experimental.list_physical_devices('GPU')\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","\n","#check current directory and import images\n","print(os.getcwd())\n","filelist=glob.glob('/content/drive/MyDrive/OASIS/keras_png_slices_train/*.png')\n","train_size = len(filelist)\n","images0=np.array([np.array(Image.open(i),dtype=\"float32\") for i in filelist[0:train_size]])\n","print('training images',images0.shape)\n","\n","filelist=glob.glob('/content/drive/MyDrive/OASIS/keras_png_slices_test/*.png')\n","test_size = len(filelist)\n","images1=np.array([np.array(Image.open(i),dtype=\"float32\") for i in filelist[0:test_size]])\n","print('test images',images1.shape)\n","\n","filelist=glob.glob('/content/drive/MyDrive/OASIS/keras_png_slices_validate/*.png')\n","val_size = len(filelist)\n","images2=np.array([np.array(Image.open(i),dtype=\"float32\") for i in filelist[0:val_size]])\n","print('validation images',images2.shape)\n","\n","#concatenate all images into one array called \"images\"\n","images=np.concatenate((images0,images1,images2), axis=0)\n","print(images.shape)\n","\n","#######################################################################\n","#Preprocessing\n","#normalise pixel values from [0,255] to [-1,1]\n","images=(images - 127.5) / 127.5\n","\n","#make into 4D array\n","images=images[:,:,:,np.newaxis]\n","\n","#check shape\n","print(images.shape)\n","\n","#######################################################################\n","#Check the brains, plot the first 10\n","pyplot.figure(figsize=(25,25))\n","for i in range(10):\n","    # define subplot\n","    pyplot.subplot(5, 5, 1 + i)\n","    # turn off axis\n","    pyplot.axis('off')\n","    # plot raw pixel data\n","    pyplot.imshow(images[i,:,:,0],cmap=\"gray\")\n","pyplot.show()\n","\n","########################################################################\n","#Call the generator and discriminator models\n","\n","g_model = define_generator()\n","\n","d_model = define_discriminator()\n","\n","########Visualising generated images############\n","#choose the number of samples to visualise\n","n_samples=5\n","#define number of points in latent space\n","latent_dim=256\n","\n","\n","#generate noise according to number of samples specified with latent_dim previously defined as 256\n","noise = tf.random.normal([n_samples, latent_dim])\n","\n","#generate fake images\n","x_fake = g_model(noise,training=False)\n","pyplot.figure(figsize=(25,25))\n","for i in range(n_samples):\n","    # define subplot\n","    pyplot.subplot(5, 5, 1 + i)\n","    pyplot.axis('off')\n","    # plot single image\n","    pyplot.imshow(x_fake[i, :, :,0],cmap='gray')\n","pyplot.show()\n","pyplot.close()\n","\n","########################################################################\n","#Define loss functions\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","########################################################################\n","#Define optimisers\n","\n","generator_optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n","\n","discriminator_optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n","\n","########################################################################\n","#Define training function\n","batch_size = 16\n","\n","#Training function\n","@tf.function\n","def train_step(images):\n","    noise = tf.random.normal([batch_size, latent_dim])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = g_model(noise, training=True)\n","\n","        real_output = d_model(images, training=True)\n","        fake_output = d_model(generated_images, training=True)\n","\n","        g_loss = generator_loss(fake_output)\n","        d_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_of_generator = gen_tape.gradient(g_loss, g_model.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(d_loss, d_model.trainable_variables)\n","\n","    generator_optimiser.apply_gradients(zip(gradients_of_generator, g_model.trainable_variables))\n","    discriminator_optimiser.apply_gradients(zip(gradients_of_discriminator, d_model.trainable_variables))\n","    return d_loss, g_loss\n","\n","#########################################################################\n","#Define training loop\n","EPOCHS = 40\n","batch_per_epoch=np.round(images.shape[0]/batch_size)\n","\n","d_loss_history_batch = []\n","g_loss_history_batch = []\n","d_loss_history_epoch = []\n","g_loss_history_epoch = []\n","\n","#number of sample images to display\n","n_samples=5\n","\n","total_size=images.shape[0]\n","\n","\n","# Batch and shuffle the data\n","train_dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(total_size).batch(batch_size)\n","\n","def train(dataset, epochs):\n","    batch_counter = 0\n","    for epoch in range(epochs):\n","        count=0\n","        epoch_d_loss = []\n","        epoch_g_loss = []\n","\n","        for image_batch in dataset:\n","            d_loss,g_loss=train_step(image_batch)\n","\n","            d_loss_history_batch.append(d_loss.numpy())\n","            g_loss_history_batch.append(g_loss.numpy())\n","            epoch_d_loss.append(d_loss.numpy())\n","            epoch_g_loss.append(g_loss.numpy())\n","\n","            if (count) % 25 == 0:\n","                print('>%d, %d/%d, d=%.8f, g=%.8f' % (epoch, count, batch_per_epoch, d_loss, g_loss))\n","            if (count) % 700 == 0:\n","                noise = tf.random.normal([n_samples, latent_dim])\n","                x_fake = g_model(noise,training=False)\n","                pyplot.figure(figsize=(25,25))\n","                for i in range(n_samples):\n","                    # define subplot\n","                    pyplot.subplot(5, 5, 1 + i)\n","                    pyplot.axis('off')\n","                    # plot single image\n","                    pyplot.imshow(x_fake[i, :, :,0],cmap='gray')\n","                # pyplot.savefig('0511 Epoch{0} batch{1}.png'.format(epoch,count))\n","                pyplot.show()\n","\n","                pyplot.close()\n","                #just save one model version per epoch\n","                # filename = 'generator_model_%03d.h5' % (epoch)\n","                # g_model.save(filename)\n","            count=count+1\n","            batch_counter += 1\n","      # 计算并记录每个epoch的平均损失\n","        avg_d_loss = np.mean(epoch_d_loss)\n","        avg_g_loss = np.mean(epoch_g_loss)\n","        d_loss_history_epoch.append(avg_d_loss)\n","        g_loss_history_epoch.append(avg_g_loss)\n","        print(f'Epoch {epoch} completed - Avg Discriminator Loss: {avg_d_loss:.4f}, Avg Generator Loss: {avg_g_loss:.4f}')\n","\n","train(train_dataset, EPOCHS)\n","\n","#######look at generator images########\n","n_samples=5\n","noise = tf.random.normal([n_samples, latent_dim])\n","x_fake = g_model(noise,training=False)\n","\n","pyplot.figure(figsize=(25,25))\n","for i in range(n_samples):\n","    # define subplot\n","    pyplot.subplot(5, 5, 1 + i)\n","    pyplot.axis('off')\n","    # plot single image\n","    pyplot.imshow(x_fake[i, :, :,0],cmap='gray')\n","pyplot.show()\n","pyplot.close()\n","\n","####################################\n","fig, (ax1, ax2) = pyplot.subplots(2, 1, figsize=(15, 16))\n","\n","    # Loss function for each batch\n","ax1.plot(d_loss_history_batch, label='Discriminator Loss (per batch)')\n","ax1.plot(g_loss_history_batch, label='Generator Loss (per batch)')\n","ax1.set_title('Training Loss per Batch')\n","ax1.set_xlabel('Batch Number')\n","ax1.set_ylabel('Loss')\n","ax1.legend()\n","ax1.grid(True)\n","\n","# Loss function for each epoch\n","ax2.plot(d_loss_history_epoch, 'o-', label='Discriminator Loss (avg per epoch)')\n","ax2.plot(g_loss_history_epoch, 'o-', label='Generator Loss (avg per epoch)')\n","ax2.set_title('Average Training Loss per Epoch')\n","ax2.set_xlabel('Epoch')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","ax2.grid(True)\n","\n","pyplot.tight_layout()\n","pyplot.savefig('training_loss_comparison.png')\n","pyplot.show()\n","\n","############SSIM#################\n","#since calculating SSIM for one image is computationally expensive, just choose the index of one image to calculate\n","#whichfake is the index of the sample image\n","whichfake=4\n","\n","#create array to store SSIM values\n","ssim_noise=[]\n","\n","#calculate SSIM for each training image\n","for i in range(images.shape[0]):\n","    ssim_noise.append( ssim(images[i,:,:,0], x_fake.numpy()[whichfake,:,:,0],\n","                      data_range=np.max(x_fake.numpy()[whichfake,:,:,0]) - np.min(x_fake.numpy()[whichfake,:,:,0])))\n","\n","#plot generated image and OASIS image that corresponds to the highest SSIM value\n","fig, axs = pyplot.subplots(2, 1, constrained_layout=True,figsize=(10,10))\n","axs[0].imshow(x_fake[whichfake, :, :, 0],cmap=\"gray\")\n","axs[0].set_title('Generated image with max SSIM: {:.4f}'.format(np.max(ssim_noise)))\n","\n","axs[1].imshow(images[ssim_noise.index(np.max(ssim_noise)), :, :, 0],cmap=\"gray\")\n","axs[1].set_title('Closest OASIS image {:.0f}'.format(ssim_noise.index(np.max(ssim_noise))))\n","\n","pyplot.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"17FHGzhj-3exwhAZea9ck4YiPPHR9uP5z"},"id":"zkvtCkwd29NR","executionInfo":{"status":"ok","timestamp":1758103149473,"user_tz":-600,"elapsed":3185893,"user":{"displayName":"Wenyue Guo","userId":"14187955803528476437"}},"outputId":"75c2f01a-d56d-4c13-b563-0078c3f279f9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}