{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBY-F7OG1C6Y",
        "outputId": "3e66a092-c7f7-491a-eda7-ad1f804e3648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Dataset sizes: Train=7731, Val=966, Test=967\n",
            "Epoch 001/50 | Train: 1058.3377 (Recon: 1016.8213, KL: 41.5164) | Val: 383.3327\n",
            "Saved best model\n",
            "Epoch 002/50 | Train: 249.8542 (Recon: 220.3515, KL: 29.5027) | Val: 169.6264\n",
            "Saved best model\n",
            "Epoch 003/50 | Train: 132.5908 (Recon: 110.5072, KL: 22.0836) | Val: 125.9742\n",
            "Saved best model\n",
            "Epoch 004/50 | Train: 110.1345 (Recon: 88.8610, KL: 21.2735) | Val: 106.7082\n",
            "Saved best model\n",
            "Epoch 005/50 | Train: 101.6709 (Recon: 81.2563, KL: 20.4145) | Val: 98.6926\n",
            "Saved best model\n",
            "Epoch 006/50 | Train: 96.5979 (Recon: 76.5166, KL: 20.0812) | Val: 95.6821\n",
            "Saved best model\n",
            "Epoch 007/50 | Train: 93.7890 (Recon: 73.8883, KL: 19.9007) | Val: 93.6851\n",
            "Saved best model\n",
            "Epoch 008/50 | Train: 90.5566 (Recon: 71.2077, KL: 19.3489) | Val: 90.1019\n",
            "Saved best model\n",
            "Epoch 009/50 | Train: 88.5290 (Recon: 69.5001, KL: 19.0289) | Val: 86.7307\n",
            "Saved best model\n",
            "Epoch 010/50 | Train: 86.4846 (Recon: 67.7098, KL: 18.7749) | Val: 86.9081\n",
            "Epoch 011/50 | Train: 84.6974 (Recon: 66.1660, KL: 18.5314) | Val: 84.3063\n",
            "Saved best model\n",
            "Epoch 012/50 | Train: 82.5149 (Recon: 64.4327, KL: 18.0821) | Val: 81.6105\n",
            "Saved best model\n",
            "Epoch 013/50 | Train: 80.4320 (Recon: 62.8940, KL: 17.5380) | Val: 79.5797\n",
            "Saved best model\n",
            "Epoch 014/50 | Train: 78.7423 (Recon: 61.5399, KL: 17.2024) | Val: 77.8927\n",
            "Saved best model\n",
            "Epoch 015/50 | Train: 77.2825 (Recon: 60.3638, KL: 16.9187) | Val: 76.0536\n",
            "Saved best model\n",
            "Epoch 016/50 | Train: 75.9629 (Recon: 59.2967, KL: 16.6662) | Val: 75.3369\n",
            "Saved best model\n",
            "Epoch 017/50 | Train: 74.5021 (Recon: 58.0120, KL: 16.4900) | Val: 73.6693\n",
            "Saved best model\n",
            "Epoch 018/50 | Train: 73.4414 (Recon: 57.2300, KL: 16.2114) | Val: 73.3455\n",
            "Saved best model\n",
            "Epoch 019/50 | Train: 72.6810 (Recon: 56.5377, KL: 16.1433) | Val: 73.1904\n",
            "Saved best model\n",
            "Epoch 020/50 | Train: 71.6639 (Recon: 55.6697, KL: 15.9942) | Val: 71.4948\n",
            "Saved best model\n",
            "Epoch 021/50 | Train: 71.0312 (Recon: 55.1181, KL: 15.9131) | Val: 70.9496\n",
            "Saved best model\n",
            "Epoch 022/50 | Train: 70.2289 (Recon: 54.4227, KL: 15.8062) | Val: 69.4055\n",
            "Saved best model\n",
            "Epoch 023/50 | Train: 69.6619 (Recon: 53.8961, KL: 15.7658) | Val: 69.4684\n",
            "Epoch 024/50 | Train: 68.9872 (Recon: 53.3138, KL: 15.6734) | Val: 70.4404\n",
            "Epoch 025/50 | Train: 68.5332 (Recon: 52.8700, KL: 15.6632) | Val: 68.4489\n",
            "Saved best model\n",
            "Epoch 026/50 | Train: 68.0466 (Recon: 52.4615, KL: 15.5851) | Val: 67.8816\n",
            "Saved best model\n",
            "Epoch 027/50 | Train: 67.6030 (Recon: 52.0343, KL: 15.5687) | Val: 67.5168\n",
            "Saved best model\n",
            "Epoch 028/50 | Train: 66.9868 (Recon: 51.5207, KL: 15.4661) | Val: 67.1969\n",
            "Saved best model\n",
            "Epoch 029/50 | Train: 66.6356 (Recon: 51.1429, KL: 15.4927) | Val: 66.6007\n",
            "Saved best model\n",
            "Epoch 030/50 | Train: 66.3223 (Recon: 50.8586, KL: 15.4636) | Val: 66.5697\n",
            "Saved best model\n",
            "Epoch 031/50 | Train: 65.9163 (Recon: 50.5033, KL: 15.4129) | Val: 65.8950\n",
            "Saved best model\n",
            "Epoch 032/50 | Train: 65.4677 (Recon: 50.0715, KL: 15.3963) | Val: 65.8981\n",
            "Epoch 033/50 | Train: 65.3899 (Recon: 49.9056, KL: 15.4843) | Val: 65.6973\n",
            "Saved best model\n",
            "Epoch 034/50 | Train: 64.8944 (Recon: 49.5219, KL: 15.3725) | Val: 65.2619\n",
            "Saved best model\n",
            "Epoch 035/50 | Train: 64.5207 (Recon: 49.1287, KL: 15.3920) | Val: 65.0085\n",
            "Saved best model\n",
            "Epoch 036/50 | Train: 64.1820 (Recon: 48.8417, KL: 15.3403) | Val: 64.7927\n",
            "Saved best model\n",
            "Epoch 037/50 | Train: 63.9822 (Recon: 48.6213, KL: 15.3609) | Val: 64.6743\n",
            "Saved best model\n",
            "Epoch 038/50 | Train: 63.8673 (Recon: 48.4981, KL: 15.3692) | Val: 64.6465\n",
            "Saved best model\n",
            "Epoch 039/50 | Train: 63.6170 (Recon: 48.2433, KL: 15.3737) | Val: 63.3247\n",
            "Saved best model\n",
            "Epoch 040/50 | Train: 63.2926 (Recon: 47.9867, KL: 15.3059) | Val: 63.7934\n",
            "Epoch 041/50 | Train: 63.1808 (Recon: 47.8510, KL: 15.3298) | Val: 62.8373\n",
            "Saved best model\n",
            "Epoch 042/50 | Train: 62.9805 (Recon: 47.6213, KL: 15.3592) | Val: 63.1823\n",
            "Epoch 043/50 | Train: 62.7153 (Recon: 47.3301, KL: 15.3852) | Val: 62.9831\n",
            "Epoch 044/50 | Train: 62.6581 (Recon: 47.3115, KL: 15.3466) | Val: 63.3401\n",
            "Epoch 045/50 | Train: 62.4295 (Recon: 47.0363, KL: 15.3933) | Val: 62.9473\n",
            "Epoch 046/50 | Train: 62.3153 (Recon: 46.8858, KL: 15.4295) | Val: 62.5771\n",
            "Saved best model\n",
            "Epoch 047/50 | Train: 62.1339 (Recon: 46.7042, KL: 15.4297) | Val: 62.2284\n",
            "Saved best model\n",
            "Epoch 048/50 | Train: 61.9475 (Recon: 46.5790, KL: 15.3686) | Val: 61.9030\n",
            "Saved best model\n",
            "Epoch 049/50 | Train: 61.7440 (Recon: 46.3261, KL: 15.4179) | Val: 62.3196\n",
            "Epoch 050/50 | Train: 61.5454 (Recon: 46.1764, KL: 15.3689) | Val: 62.0590\n",
            "Test loss: 61.7812\n",
            "Applying UMAP dimensionality reduction...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All visualizations saved!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import umap\n",
        "import time\n",
        "\n",
        "# 1. Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 2. Dataset class (optimized path handling)\n",
        "class OASISDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "\n",
        "        # Recursively search for .nii.png files in all subdirectories\n",
        "        for dirpath, _, filenames in os.walk(root_dir):\n",
        "            for f in filenames:\n",
        "                if f.lower().endswith(\".nii.png\"):\n",
        "                    self.image_paths.append(os.path.join(dirpath, f))\n",
        "\n",
        "        if not self.image_paths:\n",
        "            print(f\"Warning: No .nii.png files found in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {str(e)}\")\n",
        "            return torch.zeros(1, 128, 128)  # Return placeholder\n",
        "\n",
        "# 3. VAE model (combining best architectural features)\n",
        "class BrainVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, 2, 1),   # 128x128 -> 64x64\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),  # 64x64 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), # 32x32 -> 16x16\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1),# 16x16 -> 8x8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(256 * 8 * 8, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256 * 8 * 8, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_input = nn.Linear(latent_dim, 256 * 8 * 8)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, (256, 8, 8)),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 8x8 -> 16x16\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 16x16 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 32x32 -> 64x64\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 1, 4, 2, 1),     # 64x64 -> 128x128\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = torch.clamp(self.fc_logvar(h), -20, 20)  # Prevent extreme values\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.leaky_relu(self.decoder_input(z), 0.2)\n",
        "        return self.decoder(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# 4. Loss function (using MSE)\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
        "    batch_size = x.size(0)\n",
        "    recon_loss = F.mse_loss(recon_x, x, reduction='sum') / batch_size\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size\n",
        "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
        "\n",
        "# 5. Training function (with early stopping and model saving)\n",
        "def train_and_evaluate(data_dir, img_size=128, batch_size=16, latent_dim=256, epochs=100, lr=1e-4):\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    full_dataset = OASISDataset(data_dir, transform=transform)\n",
        "\n",
        "    # Split dataset (80% train, 10% validation, 10% test)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = int(0.1 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = BrainVAE(latent_dim=latent_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience, patience_counter = 10, 0\n",
        "    history = {'train_loss': [], 'val_loss': [], 'recon_loss': [], 'kl_loss': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, recon_loss, kl_loss = 0, 0, 0\n",
        "\n",
        "        for images in train_loader:\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            recon_images, mu, logvar = model(images)\n",
        "            loss, r_loss, k_loss = vae_loss(recon_images, images, mu, logvar)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            recon_loss += r_loss.item()\n",
        "            kl_loss += k_loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images in val_loader:\n",
        "                images = images.to(device)\n",
        "                recon_images, mu, logvar = model(images)\n",
        "                loss, _, _ = vae_loss(recon_images, images, mu, logvar)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Record history\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_recon = recon_loss / len(train_loader)\n",
        "        avg_kl = kl_loss / len(train_loader)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['recon_loss'].append(avg_recon)\n",
        "        history['kl_loss'].append(avg_kl)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
        "              f\"Train: {avg_train_loss:.4f} (Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}) | \"\n",
        "              f\"Val: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping and model saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_brain_vae.pth')\n",
        "            print(\"Saved best model\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model for testing\n",
        "    model.load_state_dict(torch.load('best_brain_vae.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    # Test set evaluation\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images in test_loader:\n",
        "            images = images.to(device)\n",
        "            recon_images, mu, logvar = model(images)\n",
        "            loss, _, _ = vae_loss(recon_images, images, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    print(f\"Test loss: {test_loss/len(test_loader):.4f}\")\n",
        "\n",
        "    return model, history, test_loader\n",
        "\n",
        "# 6. Visualization functions (combining best features)\n",
        "def visualize_results(model, test_loader, history):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # 1. Training curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.plot(history['recon_loss'], '--', label='Reconstruction Loss')\n",
        "    plt.plot(history['kl_loss'], '--', label='KL Loss')\n",
        "    plt.title('Training Curves')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Reconstruction visualization\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        images = next(iter(test_loader)).to(device)\n",
        "        recon_images, _, _ = model(images[:8])\n",
        "\n",
        "        fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
        "        for i in range(8):\n",
        "            axes[0, i].imshow(images[i].cpu().squeeze(), cmap='gray')\n",
        "            axes[0, i].set_title(f'Original {i+1}')\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "            axes[1, i].imshow(recon_images[i].cpu().squeeze(), cmap='gray')\n",
        "            axes[1, i].set_title(f'Reconstructed {i+1}')\n",
        "            axes[1, i].axis('off')\n",
        "\n",
        "        plt.suptitle('Original vs Reconstructed Images')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('reconstructions.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # 3. Generate new samples\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(16, model.latent_dim).to(device)\n",
        "        generated = model.decode(z)\n",
        "\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "        for i in range(16):\n",
        "            ax = axes[i//4, i%4]\n",
        "            ax.imshow(generated[i].cpu().squeeze(), cmap='gray')\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f'Sample {i+1}', fontsize=8)\n",
        "\n",
        "        plt.suptitle('Generated Brain MRI Samples')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('generated_samples.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # 4. Latent space UMAP visualization\n",
        "    model.eval()\n",
        "    latent_vectors = []\n",
        "    with torch.no_grad():\n",
        "        for images in test_loader:\n",
        "            images = images.to(device)\n",
        "            mu, _ = model.encode(images)\n",
        "            latent_vectors.append(mu.cpu().numpy())\n",
        "\n",
        "    latent_vectors = np.concatenate(latent_vectors)\n",
        "\n",
        "    print(\"Applying UMAP dimensionality reduction...\")\n",
        "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "    embedding = reducer.fit_transform(latent_vectors)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(embedding[:, 0], embedding[:, 1], alpha=0.6, s=10,\n",
        "                c=np.arange(len(embedding)), cmap='viridis')\n",
        "    plt.colorbar(label='Sample Index')\n",
        "    plt.title('VAE Latent Space UMAP Visualization')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.savefig('latent_space_umap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Latent space interpolation\n",
        "    with torch.no_grad():\n",
        "        img1, img2 = next(iter(test_loader))[:2].to(device)\n",
        "        mu1, _ = model.encode(img1.unsqueeze(0))\n",
        "        mu2, _ = model.encode(img2.unsqueeze(0))\n",
        "\n",
        "        fig, axes = plt.subplots(1, 10, figsize=(20, 3))\n",
        "        for i, alpha in enumerate(np.linspace(0, 1, 10)):\n",
        "            z = (1-alpha)*mu1 + alpha*mu2\n",
        "            recon = model.decode(z)\n",
        "            axes[i].imshow(recon[0].cpu().squeeze(), cmap='gray')\n",
        "            axes[i].set_title(f'α={alpha:.1f}')\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.suptitle('Latent Space Interpolation')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('latent_interpolation.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "# 7. Main function\n",
        "def main():\n",
        "    # Use OASIS dataset on Rangpur cluster\n",
        "    data_dir = \"/content/drive/MyDrive/OASIS/keras_png_slices_train\"\n",
        "\n",
        "    # Train and evaluate model\n",
        "    model, history, test_loader = train_and_evaluate(\n",
        "        data_dir,\n",
        "        img_size=128,       # Use larger image size\n",
        "        batch_size=32,       # Adjust based on GPU memory\n",
        "        latent_dim=256,      # Larger latent space dimension\n",
        "        epochs=50,\n",
        "        lr=1e-4\n",
        "    )\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_results(model, test_loader, history)\n",
        "    print(\"All visualizations saved!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fHFafdm198Wj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}